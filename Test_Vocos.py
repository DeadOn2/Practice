import torch
import torchaudio
import numpy as np
import soundfile as sf
import os
import matplotlib.pyplot as plt  # –î–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è attention
from vocos import Vocos
from speechbrain.inference.speaker import EncoderClassifier

import GigaTestLSTM
from GigaTestLSTM import Config, TextProcessor, StudentTTS, save_mel_image


# ==========================================
# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# ==========================================
def load_models(cfg):
    device = "cpu"
    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Vocos...")
    # –û–±—Ä–∞—Ç–∏ –≤–Ω–∏–º–∞–Ω–∏–µ: Vocos mel-24khz –æ–∂–∏–¥–∞–µ—Ç 100 –±–∏–Ω–æ–≤ –º–µ–ª–∞, –∫–∞–∫ —É —Ç–µ–±—è –≤ –∫–æ–Ω—Ñ–∏–≥–µ
    vocoder = Vocos.from_pretrained("charactr/vocos-mel-24khz").to(device)
    vocoder.eval()

    print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Speaker Encoder (ECAPA-TDNN)...")
    spk_encoder = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        run_opts={"device": device}
    )
    return vocoder, spk_encoder


# ==========================================
# 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞
# ==========================================
def extract_speaker_embedding(audio_path, encoder, device):
    signal, fs = torchaudio.load(audio_path)
    if fs != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
        signal = resampler(signal)
    if signal.shape[0] > 1:
        signal = torch.mean(signal, dim=0, keepdim=True)

    with torch.no_grad():
        emb = encoder.encode_batch(signal.to(device))
        return emb.squeeze(1)


# ==========================================
# 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention (–ù–û–í–û–ï)
# ==========================================
def save_attention_image(attn, path="debug_attention.png"):
    # attn: (T_decoder, T_encoder)
    plt.figure(figsize=(8, 6))
    plt.imshow(attn.cpu().numpy(), aspect='auto', origin='lower', interpolation='none')
    plt.xlabel("Encoder steps (Text)")
    plt.ylabel("Decoder steps (Audio)")
    plt.title("Attention Map")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig(path)
    plt.close()
    print(f"üîç –ö–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")


# ==========================================
# 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# ==========================================
def generate_zero_shot(
        student_model,
        vocoder,
        spk_encoder,
        text,
        ref_audio_path,
        cfg,
        processor,
        output_path="zero_shot_result.wav"
):
    student_model.eval()
    device = "cpu"

    tokens = torch.tensor([processor.encode(text)], dtype=torch.long).to(device)
    lens = torch.tensor([tokens.size(1)]).to(device)
    spk_emb = extract_speaker_embedding(ref_audio_path, spk_encoder, device)

    with torch.no_grad():
        # –¢–ï–ü–ï–†–¨ –ú–û–î–ï–õ–¨ –í–û–ó–í–†–ê–©–ê–ï–¢ 3 –ó–ù–ê–ß–ï–ù–ò–Ø
        mel_output, stop_output, attentions = student_model(tokens, lens, speaker_embs=spk_emb)

    # 1. –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º Attention (–±–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞)
    save_attention_image(attentions[0], "inference_attention.png")

    # 2. –õ–æ–≥–∏–∫–∞ –æ–±—Ä–µ–∑–∫–∏ –ø–æ Stop Token (–ù–û–í–û–ï)
    # –ò—â–µ–º –ø–µ—Ä–≤—ã–π –∫–∞–¥—Ä, –≥–¥–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ > 0.5
    # --- –ë–ï–ó–û–ü–ê–°–ù–ê–Ø –õ–û–ì–ò–ö–ê –û–ë–†–ï–ó–ö–ò ---
    stop_probs = torch.sigmoid(stop_output[0]).cpu().numpy()

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ø–æ—Ä–æ–≥ –∏ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–∞–¥—Ä–æ–≤ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 50 –∫–∞–¥—Ä–æ–≤ ~ 0.5 —Å–µ–∫)
    stop_threshold = 0.1
    min_stop_frame = 40

    # –ò—â–µ–º –∫–∞–¥—Ä—ã —Ç–æ–ª—å–∫–æ –ü–û–°–õ–ï min_stop_frame
    stop_idx = np.where(stop_probs[min_stop_frame:] > stop_threshold)[0]

    if len(stop_idx) > 0:
        # –ü—Ä–∏–±–∞–≤–ª—è–µ–º min_stop_frame, —Ç–∞–∫ –∫–∞–∫ –ø–æ–∏—Å–∫ –±—ã–ª —Å–æ —Å–º–µ—â–µ–Ω–∏–µ–º
        end_frame = stop_idx[0] + min_stop_frame
        print(f"‚úÇÔ∏è Stop Token —Å—Ä–∞–±–æ—Ç–∞–ª –Ω–∞ –∫–∞–¥—Ä–µ: {end_frame}")
        mel_output = mel_output[:, :end_frame, :]
    else:
        print("‚ö†Ô∏è Stop Token –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –∏–ª–∏ —Å—Ä–∞–±–æ—Ç–∞–ª —Å–ª–∏—à–∫–æ–º —Ä–∞–Ω–æ, –±–µ—Ä–µ–º –º–∞–∫—Å–∏–º—É–º.")
        # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ 0 –∫–∞–¥—Ä–æ–≤ –ø–æ –∫–∞–∫–æ–π-—Ç–æ –ø—Ä–∏—á–∏–Ω–µ, –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –±–µ—Ä–µ–º –≤—Å—ë
        if mel_output.shape[1] <= 1:
            print("üì¢ Force: –∏–≥–Ω–æ—Ä–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω, –º–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ –ø—É—Å—Ç–æ—Ç—É.")

    if mel_output.shape[1] == 0:
        print("‚ùå –û—à–∏–±–∫–∞: –ü—É—Å—Ç–∞—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞.")
        return

    # 3. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è Vocos
    mel = mel_output.transpose(1, 2)  # [1, 100, T]
    save_mel_image(mel, "melt_test.png")
    # –î–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–í–ê–ñ–ù–û: –ø—Ä–æ–≤–µ—Ä—å, –Ω–µ –ª–µ—Ç—è—Ç –ª–∏ —Ç—É—Ç NaN)
    mel_db = (mel * 80.0) - 80.0

    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π –∑–∞–Ω—É–ª—è–µ–º —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏–µ –∑–Ω–∞—á–µ–Ω–∏—è
    mel_db = torch.clamp(mel_db, min=-100, max=0)

    # 4. –°–∏–Ω—Ç–µ–∑
    with torch.no_grad():
        wav = vocoder.decode(mel_db)
        wav = wav.squeeze().cpu().numpy()

    sf.write(output_path, wav, 24000)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ê—É–¥–∏–æ: {output_path}")
if __name__ == "__main__":
    cfg = Config()
    # –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ä–∞–∑–º–µ—Ä —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ –≤ –∫–æ–Ω—Ñ–∏–≥–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å–æ SpeechBrain (192)
    cfg.speaker_embedding_dim = 192

    tp = TextProcessor(cfg.RUS_ALPHABET)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
    student = StudentTTS(cfg).to("cpu")

    # –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (—Ç–æ–≥–æ, —á—Ç–æ –æ–±—É—á–∞–ª—Å—è —Å X-–≤–µ–∫—Ç–æ—Ä–∞–º–∏!)
    ckpt_path = "checp_old/student_step_9500.pth"
    if os.path.exists(ckpt_path):
        ckpt = torch.load(ckpt_path, map_location="cpu")
        student.load_state_dict(ckpt['model_state_dict'])
        print(f"üöÄ –°—Ç—É–¥–µ–Ω—Ç –∑–∞–≥—Ä—É–∂–µ–Ω (Step: {ckpt.get('global_step', '?')})")
    else:
        print("‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –Ω–∞ —Å–ª—É—á–∞–π–Ω—ã—Ö –≤–µ—Å–∞—Ö.")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–æ–∫–æ–¥–µ—Ä–∞ –∏ —ç–Ω–∫–æ–¥–µ—Ä–∞ —Å–ø–∏–∫–µ—Ä–∞
    vocoder, spk_encoder = load_models(cfg)

    # --- –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø ---
    my_text = "–≠—Ç–æ—Ç –≥–æ–ª–æ—Å —Å–∫–∏–±–∏–¥–∏ –¥–æ–ø –¥–æ–ø –µ—Å"

    # –ü—É—Ç—å –∫ –ª—é–±–æ–º—É —Ñ–∞–π–ª—É, —á–µ–π –≥–æ–ª–æ—Å —Ö–æ—Ç–∏–º —É–∫—Ä–∞—Å—Ç—å
    reference_wav = "samples/audio_2026-02-16_01-29-54.wav"

    generate_zero_shot(
        student,
        vocoder,
        spk_encoder,
        my_text,
        reference_wav,
        cfg,
        tp,
        "cloned_voice.wav"
    )