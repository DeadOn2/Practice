import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import json
from pathlib import Path
from torch.utils.tensorboard import SummaryWriter
import os
from vocos import Vocos  # <--- –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–µ–º Vocos –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
from speechbrain.inference.speaker import EncoderClassifier


# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –≥–æ–ª–æ—Å–∞ (—Å–∫–∞—á–∞–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)
# –ò—Å–ø–æ–ª—å–∑—É–µ–º ECAPA-TDNN, –æ–Ω –≤—ã–¥–∞–µ—Ç –≤–µ–∫—Ç–æ—Ä —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é 192
print("–ó–∞–≥—Ä—É–∑–∫–∞ Speaker Encoder (ECAPA-TDNN)...")
spk_classifier = EncoderClassifier.from_hparams(
    source="speechbrain/spkrec-ecapa-voxceleb",
    run_opts={"device": "cuda" if torch.cuda.is_available() else "cpu"}
)
# ==========================================
# 1. Hyperparameters & Config
# ==========================================
class PodcastDistillDataset(Dataset):
    def __init__(self, root_dir, text_processor, cfg):
        self.root_dir = Path(root_dir)
        self.tp = text_processor
        self.cfg = cfg
        self.samples = []

        # –î–ª—è –ø–æ–¥—Å—Ç—Ä–∞—Ö–æ–≤–∫–∏ (–µ—Å–ª–∏ –Ω–µ—Ç .pt), –∑–∞–≥—Ä—É–∂–∞–µ–º Vocos –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–∞ –ª–µ—Ç—É
        # –ù–æ –ª—É—á—à–µ, —á—Ç–æ–±—ã –≤—Å–µ —Ñ–∞–π–ª—ã –±—ã–ª–∏ –ø—Ä–µ–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω—ã —Ç–≤–æ–∏–º —Å–∫—Ä–∏–ø—Ç–æ–º!
        self.vocos_feature_extractor = Vocos.from_pretrained("charactr/vocos-mel-24khz").to("cpu")

        print(f"–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏: {root_dir}...")
        folders = [f for f in self.root_dir.iterdir() if f.is_dir()]

        for folder in folders:
            json_files = list(folder.glob("*.json"))
            if not json_files: continue

            with open(json_files[0], 'r', encoding='utf-8') as f:
                metadata_list = json.load(f)

            for i, entry in enumerate(metadata_list):
                audio_filename = f"{folder.name}_{i}.mp3"
                audio_path = folder / audio_filename

                # –ú—ã –∏—â–µ–º –∏–º–µ–Ω–Ω–æ —Ç–æ—Ç —Ñ–∞–π–ª _teacher.pt, –∫–æ—Ç–æ—Ä—ã–π —Ç—ã —Å–æ–∑–¥–∞–ª —Å–∫—Ä–∏–ø—Ç–æ–º —Å Vocos
                teacher_mel_path = folder / f"{folder.name}_{i}_teacher.pt"
                spk_emb_path = folder / f"{folder.name}_{i}_spk.pt"

                if len(entry["text"]) > 182: continue

                # –î–æ–±–∞–≤–ª—è–µ–º –≤ —Å–ø–∏—Å–æ–∫ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –∞—É–¥–∏–æ
                if audio_path.exists():
                    self.samples.append({
                        "text": entry["text"],
                        "audio_path": str(audio_path),
                        "teacher_mel_path": str(teacher_mel_path),
                        "spk_emb_path": str(spk_emb_path),
                    })

        print(f"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.samples)} –ø—Ä–∏–º–µ—Ä–æ–≤.")

    def _get_mel_from_audio(self, audio_path):
        # –†–µ–∑–µ—Ä–≤–Ω—ã–π –º–µ—Ç–æ–¥: –µ—Å–ª–∏ .pt —Ñ–∞–π–ª–∞ –Ω–µ—Ç, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º —á–µ—Ä–µ–∑ Vocos –Ω–∞ –ª–µ—Ç—É
        # –í–ê–ñ–ù–û: –ù–∏–∫–∞–∫–æ–π –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏ / 80. Vocos —Å–∞–º –∑–Ω–∞–µ—Ç, —á—Ç–æ –¥–µ–ª–∞—Ç—å.
        wav, sr = torchaudio.load(audio_path)
        if sr != self.cfg.sample_rate:
            wav = torchaudio.functional.resample(wav, sr, self.cfg.sample_rate)

        # Vocos –æ–∂–∏–¥–∞–µ—Ç [1, Time], –µ—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ - —É—Å—Ä–µ–¥–Ω—è–µ–º
        if wav.shape[0] > 1:
            wav = wav.mean(dim=0, keepdim=True)

        with torch.no_grad():
            mel = self.vocos_feature_extractor.feature_extractor(wav)  # [1, 100, Time]

        return mel.squeeze(0).transpose(0, 1)  # [Time, 100]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        text_tokens = torch.tensor(self.tp.encode(sample["text"]), dtype=torch.long)

        # 1. –ó–∞–≥—Ä—É–∑–∫–∞ Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã (Target)
        if os.path.exists(sample["teacher_mel_path"]):
            # –ì—Ä—É–∑–∏–º –≥–æ—Ç–æ–≤—ã–π —Ç–µ–Ω–∑–æ—Ä [Time, 100], –∫–æ—Ç–æ—Ä—ã–π —Ç—ã —Å–æ–∑–¥–∞–ª
            target_mel = torch.load(sample["teacher_mel_path"])
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç —Ñ–∞–π–ª–∞, –≥–µ–Ω–µ—Ä–∏–º (–º–µ–¥–ª–µ–Ω–Ω–æ)
            target_mel = self._get_mel_from_audio(sample["audio_path"])

        # 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–∞ –≥–æ–ª–æ—Å–∞ (Speaker Embedding)
        if os.path.exists(sample["spk_emb_path"]):
            spk_emb = torch.load(sample["spk_emb_path"])
        else:
            signal, fs = torchaudio.load(sample["audio_path"])
            # SpeechBrain —Ç—Ä–µ–±—É–µ—Ç 16000Hz! –†–µ—Å–µ–º–ø–ª–∏–º —Å 24000 (–∏–ª–∏ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ)
            if fs != 16000:
                resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
                signal = resampler(signal)

            with torch.no_grad():
                spk_emb = spk_classifier.encode_batch(signal)
                spk_emb = spk_emb.squeeze(0).squeeze(0)  # (192,)

            torch.save(spk_emb, sample["spk_emb_path"])

        return text_tokens, target_mel, sample["text"], sample["audio_path"], spk_emb

import matplotlib.pyplot as plt

class AudioNormalizer:
    def __init__(self):
        # –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã –¥–ª—è Vocos (–ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–µ, –º–æ–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å –Ω–∞ —Å–≤–æ–µ–º –¥–∞—Ç–∞—Å–µ—Ç–µ)
        self.mean = -4.0
        self.std = 4.0

    def normalize(self, mel):
        return (mel - self.mean) / self.std

    def denormalize(self, mel):
        return (mel * self.std) + self.mean

normalizer = AudioNormalizer()

def save_mel_image(mel, path="mel_spectrogram.png"):
    # –ï—Å–ª–∏ —ç—Ç–æ —Ç–µ–Ω–∑–æ—Ä PyTorch, –ø–µ—Ä–µ–Ω–æ—Å–∏–º –Ω–∞ CPU –∏ –ø—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ numpy
    if torch.is_tensor(mel):
        mel = mel.detach().cpu().numpy()

    # –ï—Å–ª–∏ –ø—Ä–∏—à–µ–ª –±–∞—Ç—á [1, n_mels, Time], —É–±–∏—Ä–∞–µ–º –ª–∏—à–Ω—é—é —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å
    if len(mel.shape) == 3:
        mel = mel[0]

    plt.figure(figsize=(10, 4))
    # –í–∞–∂–Ω–æ:imshow –æ–∂–∏–¥–∞–µ—Ç (–≤—ã—Å–æ—Ç–∞, —à–∏—Ä–∏–Ω–∞), —Ç.–µ. (n_mels, Time)
    plt.imshow(mel, aspect='auto', origin='lower')
    plt.colorbar(format='%+2.0f dB')
    plt.title("Generated Mel-Spectrogram")
    plt.tight_layout()
    plt.savefig(path)
    plt.close()
    print(f"üñº –°–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –∫–∞–∫ {path}")
# –í—ã–∑—ã–≤–∞–π —ç—Ç–æ –ø–æ—Å–ª–µ –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏:
# save_mel_image(mel, "debug_mel.png")

class Config:
    # –ê–ª—Ñ–∞–≤–∏—Ç (–æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å)
    RUS_ALPHABET = " –∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è.,!?-‚Äì"
    vocab_size = len(RUS_ALPHABET) + 1

    speaker_embedding_dim = 192

    embedding_dim = 256
    encoder_hidden = 256
    decoder_hidden = 256
    attention_dim = 256

    # --- –ò–ó–ú–ï–ù–ï–ù–ò–Ø –ü–û–î VOCOS ---
    n_mels = 100  # Vocos –∏—Å–ø–æ–ª—å–∑—É–µ—Ç 100 –ø–æ–ª–æ—Å
    sample_rate = 24000  # Vocos —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ 24–∫–ì—Ü
    hop_length = 256  # –°—Ç–∞–Ω–¥–∞—Ä—Ç –¥–ª—è Vocos 24khz
    # ---------------------------

    alpha = 0.7  # –í–µ—Å MSE
    beta = 0.3  # –í–µ—Å L1

    lr = 1e-4
    batch_size = 16
    epochs = 200
    device = torch.device("cuda")


# ==========================================
# 2. Text Preprocessing Utility
# ==========================================
class TextProcessor:
    def __init__(self, alphabet):
        self.char_to_id = {char: i + 1 for i, char in enumerate(alphabet)}
        self.id_to_char = {i + 1: char for i, char in enumerate(alphabet)}
        self.pad_id = 0

    def encode(self, text):
        text = text.lower()
        return [self.char_to_id[c] for c in text if c in self.char_to_id]

    def decode(self, ids):
        return "".join([self.id_to_char[i] for i in ids if i in self.id_to_char])
class LocationSensitiveAttention(nn.Module):
    def __init__(self, encoder_dim, decoder_dim, attention_dim, attention_location_n_filters=32,
                 attention_location_kernel_size=31):
        super().__init__()
        self.W1 = nn.Linear(encoder_dim, attention_dim, bias=False)
        self.W2 = nn.Linear(decoder_dim, attention_dim, bias=False)
        self.V = nn.Linear(attention_dim, 1, bias=False)

        # –°–≤–µ—Ä—Ç–∫–∞ —Å–º–æ—Ç—Ä–∏—Ç –Ω–∞ —Ç–æ, –∫—É–¥–∞ –º–æ–¥–µ–ª—å —Å–º–æ—Ç—Ä–µ–ª–∞ –Ω–∞ –ø—Ä–æ—à–ª–æ–º —à–∞–≥–µ
        padding = attention_location_kernel_size // 2
        self.location_conv = nn.Conv1d(
            in_channels=1,
            out_channels=attention_location_n_filters,
            kernel_size=attention_location_kernel_size,
            padding=padding,
            bias=False
        )
        self.location_dense = nn.Linear(attention_location_n_filters, attention_dim, bias=False)

    def forward(self, query, keys, prev_weights, mask=None):
        # query: (B, 1, dec_dim), keys: (B, T, enc_dim), prev_weights: (B, T)
        proj_key = self.W1(keys)
        proj_query = self.W2(query)  # –¢—Ä–∞–Ω—Å–ª–∏—Ä—É–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –±–ª–∞–≥–æ–¥–∞—Ä—è –±—Ä–æ–¥–∫–∞—Å—Ç–∏–Ω–≥—É

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –ª–æ–∫–∞—Ü–∏–∏ –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –≤–µ—Å–æ–≤ –≤–Ω–∏–º–∞–Ω–∏—è
        loc_feat = self.location_conv(prev_weights.unsqueeze(1))  # (B, filters, T)
        loc_feat = loc_feat.transpose(1, 2)  # (B, T, filters)
        proj_loc = self.location_dense(loc_feat)  # (B, T, attention_dim)

        # –°–∫–ª–∞–¥—ã–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç + –∑–∞–ø—Ä–æ—Å + –ø–æ–∑–∏—Ü–∏—é
        scores = self.V(torch.tanh(proj_key + proj_query + proj_loc)).squeeze(-1)

        if mask is not None:
            scores = scores.masked_fill(mask == 0, -1e9)

        weights = F.softmax(scores, dim=-1)
        context = torch.bmm(weights.unsqueeze(1), keys)
        return context, weights

# ==========================================
# 4. Encoder Module
# ==========================================
class Encoder(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )

    def forward(self, x, lengths):
        x = self.embedding(x)
        packed_x = nn.utils.rnn.pack_padded_sequence(
            x, lengths.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_outputs, _ = self.lstm(packed_x)
        outputs, _ = nn.utils.rnn.pad_packed_sequence(packed_outputs, batch_first=True)
        return outputs  # (B, T, hidden_dim * 2)


# ==========================================
# 5. Decoder Module
# ==========================================
def guided_attention_loss(attentions, text_lens, mel_lens, g=0.2):
    """
    attentions: —Ç–µ–Ω–∑–æ—Ä —Ñ–æ—Ä–º—ã (B, T_dec, T_enc)
    text_lens: —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–æ–≤ (B)
    mel_lens: —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º (B)
    """
    B, T_dec, T_enc = attentions.size()
    device = attentions.device
    loss = 0.0

    for i in range(B):
        N = text_lens[i].item()  # –†–µ–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ (–±–µ–∑ –ø–∞–¥–¥–∏–Ω–≥–∞)
        M = mel_lens[i].item()  # –†–µ–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞—É–¥–∏–æ (–±–µ–∑ –ø–∞–¥–¥–∏–Ω–≥–∞)

        if N == 0 or M == 0: continue

        # –°–æ–∑–¥–∞–µ–º —Å–µ—Ç–∫—É –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        grid_n, grid_m = torch.meshgrid(
            torch.arange(N, device=device),
            torch.arange(M, device=device),
            indexing='ij'
        )

        # –ú–∞—Ç—Ä–∏—Ü–∞ —à—Ç—Ä–∞—Ñ–æ–≤: 0 –Ω–∞ –¥–∏–∞–≥–æ–Ω–∞–ª–∏, –±–ª–∏–∑–∫–æ –∫ 1 –ø–æ –∫—Ä–∞—è–º
        W = 1.0 - torch.exp(-((grid_n.float() / N - grid_m.float() / M) ** 2) / (2 * g ** 2))
        W = W.T  # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å —Ñ–æ—Ä–º—É (M, N)

        # –£–º–Ω–æ–∂–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω–æ–µ –≤–Ω–∏–º–∞–Ω–∏–µ –Ω–∞ –º–∞—Ç—Ä–∏—Ü—É —à—Ç—Ä–∞—Ñ–æ–≤
        # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∞–ª—å–Ω—ã–µ –¥–ª–∏–Ω—ã, –∏–≥–Ω–æ—Ä–∏—Ä—É—è –ø–∞–¥–¥–∏–Ω–≥–∏
        attn_slice = attentions[i, :M, :N]
        loss += torch.mean(attn_slice * W)

    return loss / B

class PreNet(nn.Module):
    def __init__(self, in_dim, sizes=[256, 256]):
        super().__init__()
        self.layer1 = nn.Linear(in_dim, sizes[0])
        self.layer2 = nn.Linear(sizes[0], sizes[1])

    def forward(self, x):
        # –í–ê–ñ–ù–û: training=True —Å—Ç–æ–∏—Ç –ñ–ï–°–¢–ö–û. Dropout –¥–æ–ª–∂–µ–Ω —Ä–∞–±–æ—Ç–∞—Ç—å –≤—Å–µ–≥–¥–∞!
        x = F.dropout(F.relu(self.layer1(x)), p=0.5, training=True)
        x = F.dropout(F.relu(self.layer2(x)), p=0.5, training=True)
        return x


class PostNet(nn.Module):
    def __init__(self, n_mels=100, postnet_embedding_dim=512, kernel_size=5, dropout=0.1):
        super().__init__()

        self.convolutions = nn.ModuleList()

        # –ü–µ—Ä–≤—ã–π —Å–ª–æ–π (in: n_mels, out: 512)
        self.convolutions.append(
            nn.Sequential(
                nn.Conv1d(n_mels, postnet_embedding_dim, kernel_size, stride=1, padding=int((kernel_size - 1) / 2)),
                nn.BatchNorm1d(postnet_embedding_dim)
            )
        )

        # –°—Ä–µ–¥–Ω–∏–µ 3 —Å–ª–æ—è (in: 512, out: 512)
        for _ in range(3):
            self.convolutions.append(
                nn.Sequential(
                    nn.Conv1d(postnet_embedding_dim, postnet_embedding_dim, kernel_size, stride=1,
                              padding=int((kernel_size - 1) / 2)),
                    nn.BatchNorm1d(postnet_embedding_dim)
                )
            )

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π (in: 512, out: n_mels) - –ë–ï–ó –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ –≤ –∫–æ–Ω—Ü–µ
        self.convolutions.append(
            nn.Sequential(
                nn.Conv1d(postnet_embedding_dim, n_mels, kernel_size, stride=1, padding=int((kernel_size - 1) / 2)),
                nn.BatchNorm1d(n_mels)
            )
        )

        self.dropout = dropout

    def forward(self, x):
        # x –ø—Ä–∏—Ö–æ–¥–∏—Ç –∏–∑ –¥–µ–∫–æ–¥–µ—Ä–∞ —Å —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é [Batch, Time, Mels]
        # –°–≤–µ—Ä—Ç–∫–∏ Conv1d –æ–∂–∏–¥–∞—é—Ç —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å [Batch, Mels, Time]
        x = x.transpose(1, 2)

        for i in range(len(self.convolutions) - 1):
            x = F.dropout(torch.tanh(self.convolutions[i](x)), p=self.dropout, training=self.training)

        # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Å–ª–æ–π –±–µ–∑ Tanh
        x = F.dropout(self.convolutions[-1](x), p=self.dropout, training=self.training)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∫ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ [Batch, Time, Mels]
        x = x.transpose(1, 2)
        return x

class Decoder(nn.Module):
    def __init__(self, n_mels, decoder_hidden, encoder_total_dim, attention_dim, speaker_dim):
        super().__init__()
        self.n_mels = n_mels
        self.decoder_hidden = decoder_hidden

        # –ù–û–í–û–ï: –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Pre-Net
        self.prenet = PreNet(n_mels, [256, 256])

        # –û–±–Ω–æ–≤–ª—è–µ–º —Ä–∞–∑–º–µ—Ä –≤—Ö–æ–¥–∞: –≤—ã—Ö–æ–¥ Pre-Net (256) + –∫–æ–Ω—Ç–µ–∫—Å—Ç + —Å–ø–∏–∫–µ—Ä
        self.lstm_input_size = 256 + encoder_total_dim + speaker_dim
        self.lstm = nn.LSTMCell(self.lstm_input_size, decoder_hidden)

        self.attention = LocationSensitiveAttention(encoder_total_dim, decoder_hidden, attention_dim)

        self.linear_input_size = decoder_hidden + encoder_total_dim + speaker_dim
        self.linear = nn.Linear(self.linear_input_size, n_mels)
        self.stop_linear = nn.Linear(self.linear_input_size, 1)

    def forward(self, encoder_outputs, encoder_mask, spk_emb, teacher_mels=None, max_len=1000):
        batch_size = encoder_outputs.size(0)
        device = encoder_outputs.device
        prev_weights = torch.zeros(batch_size, encoder_outputs.size(1)).to(device)
        h = torch.zeros(batch_size, self.decoder_hidden).to(device)
        c = torch.zeros(batch_size, self.decoder_hidden).to(device)
        mel_input = torch.zeros(batch_size, self.n_mels).to(device)

        outputs = []
        stop_tokens = []
        attentions = []

        steps = teacher_mels.size(1) if teacher_mels is not None else max_len

        for t in range(steps):
            # 1. Attention
            context, attn_weights = self.attention(h.unsqueeze(1), encoder_outputs, prev_weights, encoder_mask)
            context = context.squeeze(1)

            attentions.append(attn_weights.squeeze(1))

            prev_weights = attn_weights.squeeze(1)  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —à–∞–≥–∞

            # –ù–û–í–û–ï: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º mel_input —á–µ—Ä–µ–∑ Pre-Net
            prenet_out = self.prenet(mel_input)

            # 2. –®–∞–≥ LSTM (–∏—Å–ø–æ–ª—å–∑—É–µ–º prenet_out –≤–º–µ—Å—Ç–æ —Å—ã—Ä–æ–≥–æ mel_input)
            rnn_input = torch.cat([prenet_out, context, spk_emb], dim=-1)
            h, c = self.lstm(rnn_input, (h, c))

            # 3. –§–æ—Ä–º–∏—Ä—É–µ–º –≤—ã—Ö–æ–¥
            concat_out = torch.cat([h, context, spk_emb], dim=-1)

            mel_out = self.linear(concat_out)
            stop_out = self.stop_linear(concat_out)

            outputs.append(mel_out)
            stop_tokens.append(stop_out)

            # Teacher forcing
            # Teacher forcing c –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é (Scheduled Sampling)
            if teacher_mels is None:
                # –ë–µ—Ä–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–∞ —á–µ—Ä–µ–∑ —Å–∏–≥–º–æ–∏–¥—É
                stop_prob = torch.sigmoid(stop_out)[0].item()

                # –£—Å–ª–æ–≤–∏—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏:
                # - –ü—Ä–æ—à–ª–æ —Ö–æ—Ç—è –±—ã 20-30 –∫–∞–¥—Ä–æ–≤ (—á—Ç–æ–±—ã –Ω–µ —É–ø–∞—Å—Ç—å –≤ —Å–∞–º–æ–º –Ω–∞—á–∞–ª–µ)
                # - –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –∫–æ–Ω—Ü–∞ > 0.5
                if t > 30 and stop_prob > 0.5:
                    print(f"DEBUG: –ú–æ–¥–µ–ª—å —Ä–µ—à–∏–ª–∞ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç—å—Å—è –Ω–∞ —à–∞–≥–µ {t}")
                    break

            if teacher_mels is not None:
                if t < teacher_mels.size(1) - 1:
                    # –ù–û–í–û–ï: –° –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å—é 25% –∑–∞—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–≤–æ–π –∂–µ –≤—ã—Ö–æ–¥
                    # (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –º—ã –≤ —Ä–µ–∂–∏–º–µ –æ–±—É—á–µ–Ω–∏—è –∏ —ç—Ç–æ –Ω–µ –ø–µ—Ä–≤—ã–π —à–∞–≥)
                    if self.training and t > 0 and torch.rand(1).item() < 0.25:
                        mel_input = mel_out.detach()  # –û—Ç—Ä—ã–≤–∞–µ–º –æ—Ç –≥—Ä–∞—Ñ–∞, —á—Ç–æ–±—ã –Ω–µ –≤–∑–æ—Ä–≤–∞—Ç—å –ø–∞–º—è—Ç—å
                    else:
                        mel_input = teacher_mels[:, t, :]
                else:
                    mel_input = mel_out
            else:
                mel_input = mel_out

        return torch.stack(outputs, dim=1), torch.stack(stop_tokens, dim=1), torch.stack(attentions, dim=1)

# ==========================================
# 6. Student TTS Model
# ==========================================
class StudentTTS(nn.Module):
    def __init__(self, cfg):
        super().__init__()
        self.encoder = Encoder(cfg.vocab_size, cfg.embedding_dim, cfg.encoder_hidden)
        self.decoder = Decoder(
            cfg.n_mels,
            cfg.decoder_hidden,
            cfg.encoder_hidden * 2,
            cfg.attention_dim,
            # –£–î–ê–õ–ò–õ–ò: num_speakers
            speaker_dim=cfg.speaker_embedding_dim
        )
        self.postnet = PostNet(n_mels=cfg.n_mels)

    def forward(self, text, text_lengths, speaker_embs, mels=None):
        device = text.device
        mask = torch.arange(text.size(1), device=device).expand(len(text_lengths),
                                                                text.size(1)) < text_lengths.unsqueeze(1)

        encoder_outputs = self.encoder(text, text_lengths)
        mel_outputs, stop_outputs, attentions = self.decoder(encoder_outputs, mask, speaker_embs, teacher_mels=mels)

        # --- –ù–û–í–û–ï: –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ Post-Net –∏ –ø—Ä–∏–±–∞–≤–ª—è–µ–º –∫ —Å—ã—Ä–æ–º—É –≤—ã—Ö–æ–¥—É ---
        mel_outputs_post = mel_outputs + self.postnet(mel_outputs)

        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 4 –∑–Ω–∞—á–µ–Ω–∏—è (—Å—ã—Ä–æ–π –º–µ–ª –∏ —É–ª—É—á—à–µ–Ω–Ω—ã–π –º–µ–ª)
        return mel_outputs, mel_outputs_post, stop_outputs, attentions
# ==========================================
# 8. Dataset & Collate
# ==========================================
class RussianTTSDataset(Dataset):
    def __init__(self, texts, gt_mels, teacher_mels, processor):
        self.texts = texts
        self.gt_mels = gt_mels
        self.teacher_mels = teacher_mels
        self.processor = processor

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        tokens = torch.tensor(self.processor.encode(self.texts[idx]), dtype=torch.long)
        return tokens, self.gt_mels[idx], self.teacher_mels[idx]

def save_checkpoint(model, optimizer, epoch, global_step, loss, path):
    checkpoint = {
        'epoch': epoch,
        'global_step': global_step,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
    }
    torch.save(checkpoint, path)
    print(f"--- –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {path} ---")
# ==========================================
# 3. –û–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–π Collate –∏ Training Loop
# ==========================================
def collate_fn_podcast(batch):
    # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –¥–ª—è –ø–∞–¥–∏–Ω–≥–∞ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é –¥–ª–∏–Ω—ã —Ç–µ–∫—Å—Ç–∞)
    batch.sort(key=lambda x: len(x[0]), reverse=True)

    tokens, mels, raw_texts, audio_paths, spk_embs = zip(*batch)

    token_lens = torch.tensor([len(x) for x in tokens])
    tokens_padded = nn.utils.rnn.pad_sequence(tokens, batch_first=True)

    mel_lens = torch.tensor([x.size(0) for x in mels])
    mels_padded = nn.utils.rnn.pad_sequence(mels, batch_first=True) # –ü–∞–¥–¥–∏–Ω–≥ –Ω—É–ª—è–º–∏ - –æ–∫ –¥–ª—è Vocos

    spk_embs_tensor = torch.stack(spk_embs)

    return tokens_padded, token_lens, mels_padded, mel_lens, raw_texts, audio_paths, spk_embs_tensor


def train_with_distillation(root_dir):
    # --- –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ Loss ---
    def masked_mse(preds, targets, mask):
        diff = (preds - targets) ** 2
        return (diff * mask).sum() / (mask.sum() * cfg.n_mels + 1e-8)

    def masked_l1(preds, targets, mask):
        diff = torch.abs(preds - targets)
        return (diff * mask).sum() / (mask.sum() * cfg.n_mels + 1e-8)


    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    cfg = Config()
    tp = TextProcessor(cfg.RUS_ALPHABET)

    dataset = PodcastDistillDataset(root_dir, tp, cfg)
    dataloader = DataLoader(dataset, batch_size=cfg.batch_size, collate_fn=collate_fn_podcast, shuffle=True)

    student = StudentTTS(cfg).to(cfg.device)
    optimizer = torch.optim.AdamW(student.parameters(), lr=cfg.lr)

    writer = SummaryWriter(log_dir="runs/fast_distill_v2")  # v2 —á—Ç–æ–±—ã –Ω–µ –ø—É—Ç–∞—Ç—å –ª–æ–≥–∏


    # Loss –¥–ª—è Stop Token (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)
    # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –≤–µ—Å –¥–æ 15.0, —á—Ç–æ–±—ã –º–æ–¥–µ–ª—å –•–û–¢–ï–õ–ê –Ω–∞–π—Ç–∏ –∫–æ–Ω–µ—Ü, –Ω–æ –Ω–µ —Å—Ä–∞–∑—É
    bce_loss = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([15.0]).to(cfg.device), reduction='none')
    # pos_weight=5.0 –ø–æ–º–æ–≥–∞–µ—Ç, —Ç–∞–∫ –∫–∞–∫ –∫–∞–¥—Ä–æ–≤ "–∫–æ–Ω—Ü–∞" –æ—á–µ–Ω—å –º–∞–ª–æ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å "–Ω–µ –∫–æ–Ω—Ü–æ–º"

    global_step = 0
    start_epoch = 0

    # --- –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å) ---
    checkpoint_dir = "checkpoints"
    os.makedirs(checkpoint_dir, exist_ok=True)

    checkpoints = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pth') and 'step' in f]
    if checkpoints:
        checkpoints.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))
        last_checkpoint = os.path.join(checkpoint_dir, checkpoints[-1])
        print(f"–ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞: {last_checkpoint}")
        try:
            ckpt = torch.load(last_checkpoint)

            # 1. –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –ß–ê–°–¢–ò–ß–ù–û (Post-Net –æ—Å—Ç–∞–Ω–µ—Ç—Å—è —Ä–∞–Ω–¥–æ–º–Ω—ã–º)
            student.load_state_dict(ckpt['model_state_dict'], strict=False)

            # 2. –í–ê–ñ–ù–û: –ö–æ–º–º–µ–Ω—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–∞!
            optimizer.load_state_dict(ckpt['optimizer_state_dict'])

            # –ü–†–ò–ù–£–î–ò–¢–ï–õ–¨–ù–û –æ–±–Ω–æ–≤–ª—è–µ–º LR –¥–ª—è –≤—Å–µ—Ö –≥—Ä—É–ø–ø –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
            new_lr = 5e-5
            for param_group in optimizer.param_groups:
                param_group['lr'] = new_lr
            print(f"üìâ Learning Rate –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤: {new_lr}")

            # 3. –ù–æ —à–∞–≥ –∏ —ç–ø–æ—Ö—É –æ—Å—Ç–∞–≤–ª—è–µ–º, —á—Ç–æ–±—ã –≥—Ä–∞—Ñ–∏–∫–∏ –≤ TensorBoard –Ω–µ —Å–∫–ª–µ–∏–ª–∏—Å—å
            global_step = ckpt.get('global_step', 0)
            start_epoch = ckpt.get('epoch', 0)

            print(f"‚úÖ –£—Å–ø–µ—Ö! –ú–æ–¥–µ–ª—å –ø–æ–¥—Ö–≤–∞—Ç–∏–ª–∞ —Å—Ç–∞—Ä—ã–µ –≤–µ—Å–∞. –ù–∞—á–∏–Ω–∞–µ–º —Å —à–∞–≥–∞ {global_step}")
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")

    print("–ù–∞—á–∞–ª–æ –æ–±—É—á–µ–Ω–∏—è —Å Stop Token...")

    student.train()

    try:
        for epoch in range(start_epoch, cfg.epochs):
            for batch in dataloader:
                tokens, token_lens, gts, gt_lens, raw_texts, audio_paths, speaker_ids = batch

                tokens = tokens.to(cfg.device)
                token_lens = token_lens.to(cfg.device)
                gts = gts.to(cfg.device)
                gt_lens = gt_lens.to(cfg.device)
                speaker_ids = speaker_ids.to(cfg.device)  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞ GPU

                # –í —ç—Ç–æ–º –∫–æ–¥–µ –º—ã –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ç–¥–µ–ª—å–Ω—ã–π –º–∞—Å—Å–∏–≤ teachers –¥–ª—è –ø—Ä–æ—Å—Ç–æ—Ç—ã (–¥–∏—Å—Ç–∏–ª–ª—è—Ü–∏—è –∏–∑ —Ñ–∞–π–ª–∞),
                # –Ω–æ –µ—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –æ—Ç–¥–µ–ª—å–Ω—ã–π —Ñ–∞–π–ª teacher_mel, –∑–∞–≥—Ä—É–∂–∞–π—Ç–µ –µ–≥–æ.
                # –î–ª—è –ø—Ä–∏–º–µ—Ä–∞ —Å—á–∏—Ç–∞–µ–º, —á—Ç–æ target - —ç—Ç–æ gts (–∏–ª–∏ teacher, –µ—Å–ª–∏ –≤—ã –µ–≥–æ –ø—Ä–æ–∫–∏–Ω—É–ª–∏).
                target_mels = gts

                optimizer.zero_grad()

                # 1. –ü—Ä—è–º–æ–π –ø—Ä–æ—Ö–æ–¥ (Forward)
                # Student —Ç–µ–ø–µ—Ä—å –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (mel_raw, mel_post, stop, attn)
                pred_mels_raw, pred_mels_post, pred_stops, attentions = student(
                    tokens, token_lens, speaker_embs=speaker_ids, mels=target_mels
                )

                # 2. –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω (Trim)
                min_len = min(pred_mels_raw.size(1), target_mels.size(1))

                p_mel_raw = pred_mels_raw[:, :min_len, :]
                p_mel_post = pred_mels_post[:, :min_len, :]
                t_mel = target_mels[:, :min_len, :]
                p_stop = pred_stops[:, :min_len, :]

                # 3. –ú–∞—Å–∫–∞ –¥–ª—è –∞—É–¥–∏–æ
                mask = torch.arange(min_len, device=cfg.device).expand(len(gt_lens), min_len) < gt_lens.unsqueeze(1)
                mask_expanded = mask.unsqueeze(-1).float()

                # 4. –†–∞—Å—á–µ—Ç Loss –¥–ª—è Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º (–ù–û–í–û–ï: –°—á–∏—Ç–∞–µ–º –¥–ª—è RAW –∏ –¥–ª—è POST)
                loss_mse_raw = ((p_mel_raw - t_mel) ** 2 * mask_expanded).sum() / (mask.sum() * cfg.n_mels + 1e-8)
                loss_mse_post = ((p_mel_post - t_mel) ** 2 * mask_expanded).sum() / (mask.sum() * cfg.n_mels + 1e-8)

                # L1 –æ–±—ã—á–Ω–æ —Å—á–∏—Ç–∞—é—Ç —Ç–æ–ª—å–∫–æ –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ (Post-Net) –≤—ã—Ö–æ–¥–∞
                loss_l1 = (torch.abs(p_mel_post - t_mel) * mask_expanded).sum() / (mask.sum() * cfg.n_mels + 1e-8)

                # 5. –†–∞—Å—á–µ—Ç Loss –¥–ª—è Stop Token (–û—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –±—ã–ª–æ)
                stop_targets = torch.zeros_like(p_stop)
                for i, length in enumerate(gt_lens):
                    if length < min_len:
                        stop_targets[i, length:, 0] = 1.0

                loss_stop = bce_loss(p_stop, stop_targets).mean()

                # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º Guided Loss
                loss_guide = guided_attention_loss(attentions, token_lens, gt_lens)

                # 6. –°—É–º–º–∞—Ä–Ω—ã–π Loss (–ù–û–í–û–ï: –¥–æ–±–∞–≤–ª—è–µ–º loss_mse_post)
                # –ú—ã —à—Ç—Ä–∞—Ñ—É–µ–º –º–æ–¥–µ–ª—å –∏ –∑–∞ —Å—ã—Ä–æ–π –≤—ã—Ö–æ–¥, –∏ –∑–∞ –≤—ã—Ö–æ–¥ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞
                loss = (cfg.alpha * loss_mse_raw) + (cfg.alpha * loss_mse_post) + (cfg.beta * loss_l1) + loss_stop + (
                            10.0 * loss_guide)
                loss.backward()

                # Gradient Clipping (–≤–∞–∂–Ω–æ –¥–ª—è LSTM)
                torch.nn.utils.clip_grad_norm_(student.parameters(), 1.0)

                optimizer.step()

                global_step += 1

                # –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
                if global_step % 10 == 0:
                    writer.add_scalar('Loss/Total', loss.item(), global_step)
                    writer.add_scalar('Loss/Guide', loss_guide.item(), global_step)

                    writer.add_scalar('Loss/Mel_MSE', loss_mse_post.item(), global_step)
                    writer.add_scalar('Loss/L1', loss_l1.item(), global_step)
                    writer.add_scalar('Loss/Stop_BCE', loss_stop.item(), global_step)
                    attn_matrix = attentions[0].detach().cpu().numpy()  # –§–æ—Ä–º–∞—Ç: (T_dec, T_enc)

                    fig, ax = plt.subplots(figsize=(6, 4))
                    im = ax.imshow(attn_matrix, aspect='auto', origin='lower', interpolation='none')
                    fig.colorbar(im, ax=ax)
                    plt.title(f"Attention (Step {global_step})")
                    plt.xlabel("Encoder Steps (Text)")
                    plt.ylabel("Decoder Steps (Audio)")
                    plt.tight_layout()

                    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –≤ TensorBoard –∏ –∑–∞–∫—Ä—ã–≤–∞–µ–º –≥—Ä–∞—Ñ–∏–∫
                    writer.add_figure('Attention_Alignment', fig, global_step)

                    print(
                        f"Epoch {epoch}/{cfg.epochs} | Step {global_step} | Total: {loss.item():.6f} | Mel: {loss_mse_post.item():.6f} | L1: {loss_l1.item():.6f}| Stop: {loss_stop.item():.6f} | Guide: {loss_guide.item():.6f}")

                # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
                if global_step % 250 == 0:
                    save_path = os.path.join(checkpoint_dir, f"student_step_{global_step}.pth")
                    torch.save({
                        'global_step': global_step,
                        'epoch': epoch,
                        'model_state_dict': student.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'loss': loss.item(),
                    }, save_path)
                    print(f"üíæ –ß–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {save_path}")

    except KeyboardInterrupt:
        print("\n–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –æ–±—É—á–µ–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º...")
        save_path = os.path.join(checkpoint_dir, "interrupted.pth")
        torch.save({
            'global_step': global_step,
            'epoch': epoch,
            'model_state_dict': student.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'loss': loss.item(),
        }, save_path)
        print("üíæ –ê–≤–∞—Ä–∏–π–Ω—ã–π —á–µ–∫–ø–æ–∏–Ω—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω.")

    writer.close()
    plt.close(fig)
    print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ.")


# ==========================================
# 10. Inference & Main
# ==========================================
def inference(model, text, ref_audio_path, cfg, processor):
    model.eval()
    tokens = torch.tensor([processor.encode(text)], dtype=torch.long).to(cfg.device)
    lens = torch.tensor([tokens.size(1)]).to(cfg.device)

    signal, fs = torchaudio.load(ref_audio_path)
    if fs != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
        signal = resampler(signal)

    with torch.no_grad():
        signal = signal.to(cfg.device)
        spk_emb = spk_classifier.encode_batch(signal).squeeze(0).squeeze(0)

    # 2. –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –º–æ–¥–µ–ª–∏ (–ù–û–í–û–ï: –∑–∞–±–∏—Ä–∞–µ–º –≤—Ç–æ—Ä–æ–π –∞—Ä–≥—É–º–µ–Ω—Ç)
    with torch.no_grad():
        mel_raw, mel_post, _, _ = model(tokens, lens, speaker_embs=spk_emb.unsqueeze(0))

    # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–º–µ–Ω–Ω–æ mel_post!
    return mel_post

import torchaudio

def save_audio_vocos(mel_output, filename="output_vocos.wav", device="cuda"):
    """
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ—Ç Mel-—Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—É –≤ –∑–≤—É–∫ —Å –ø–æ–º–æ—â—å—é Vocos.
    """
    # –ó–∞–≥—Ä—É–∂–∞–µ–º Vocos (–µ—Å–ª–∏ –µ—â–µ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω –≥–¥–µ-—Ç–æ –≥–ª–æ–±–∞–ª—å–Ω–æ)
    print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Vocos –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞...")
    vocoder = Vocos.from_pretrained("charactr/vocos-mel-24khz").to(device)

    # mel_output –ø—Ä–∏—Ö–æ–¥–∏—Ç –∏–∑ –º–æ–¥–µ–ª–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ [1, Time, 100]
    # Vocos –∂–¥–µ—Ç [1, 100, Time], –ø–æ—ç—Ç–æ–º—É —Ç—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º
    features = mel_output.transpose(1, 2)

    with torch.no_grad():
        wav = vocoder.decode(features)

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º
    import soundfile as sf
    sf.write(filename, wav.squeeze().cpu().numpy(), 24000)
    print(f"üéß –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {filename}")

if __name__ == "__main__":
    cfg = Config()
    tp = TextProcessor(cfg.RUS_ALPHABET)

    print("Starting training on Russian dataset...")
    train_with_distillation("C:/Users/light/Downloads/podcasts_1_stripped_archive/podcasts_1_stripped/test")
