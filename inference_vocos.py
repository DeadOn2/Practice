import torch
import torchaudio
import numpy as np
import soundfile as sf
import os
import matplotlib.pyplot as plt
from vocos import Vocos
from speechbrain.inference.speaker import EncoderClassifier

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–≤–æ–∏ –∫–ª–∞—Å—Å—ã –∏–∑ —Ñ–∞–π–ª–∞ –æ–±—É—á–µ–Ω–∏—è
# –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è GigaTestLSTM.py –∏–ª–∏ –∏–∑–º–µ–Ω–∏ –∏–º–ø–æ—Ä—Ç
from GigaTestLSTM import Config, TextProcessor, StudentTTS, AudioNormalizer

# –°–æ–∑–¥–∞–µ–º —ç–∫–∑–µ–º–ø–ª—è—Ä (–ø–∞—Ä–∞–º–µ—Ç—Ä—ã mean/std –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å –¢–ê–ö–ò–ú–ò –ñ–ï, –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
normalizer = AudioNormalizer()
# ==========================================
# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# ==========================================
def load_models(cfg, device="cpu"):
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Vocos –Ω–∞ {device}...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    vocoder = Vocos.from_pretrained("charactr/vocos-mel-24khz").to(device)
    vocoder.eval()

    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Speaker Encoder (ECAPA-TDNN) –Ω–∞ {device}...")
    spk_encoder = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        run_opts={"device": device}
    )
    return vocoder, spk_encoder


# ==========================================
# 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (SpeechBrain)
# ==========================================
def extract_speaker_embedding(audio_path, encoder, device):
    # SpeechBrain ECAPA-TDNN —Ç—Ä–µ–±—É–µ—Ç 16000 Hz
    signal, fs = torchaudio.load(audio_path)

    if fs != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
        signal = resampler(signal)

    # –ï—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ -> –º–æ–Ω–æ
    if signal.shape[0] > 1:
        signal = torch.mean(signal, dim=0, keepdim=True)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ —ç–Ω–∫–æ–¥–µ—Ä–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ)
    signal = signal / torch.max(torch.abs(signal))

    with torch.no_grad():
        # signal –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ —ç–Ω–∫–æ–¥–µ—Ä
        emb = encoder.encode_batch(signal.to(device))
        return emb.squeeze(1)  # [1, 192]


# ==========================================
# 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention
# ==========================================
def save_attention_image(attn, path="debug_attention.png"):
    plt.figure(figsize=(10, 6))
    plt.imshow(attn.cpu().numpy(), aspect='auto', origin='lower', interpolation='none')
    plt.xlabel("Encoder steps (Text)")
    plt.ylabel("Decoder steps (Audio)")
    plt.title("Attention Map")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig(path)
    plt.close()
    print(f"üîç –ö–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")


# ==========================================
# 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# ==========================================
def generate_zero_shot(
        student_model,
        vocoder,
        spk_encoder,
        text,
        ref_audio_path,
        cfg,
        processor,
        output_path="zero_shot_result.wav",
        device="cpu"
):
    student_model.eval()
    student_model.to(device)

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
    tokens = torch.tensor([processor.encode(text)], dtype=torch.long).to(device)
    lens = torch.tensor([tokens.size(1)]).to(device)

    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥–æ–ª–æ—Å–∞
    spk_emb = extract_speaker_embedding(ref_audio_path, spk_encoder, device)

    print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã (—Å —É—á–µ—Ç–æ–º Post-Net)...")
    with torch.no_grad():
        # –ù–û–í–û–ï: –†–∞—Å–ø–∞–∫–æ–≤—ã–≤–∞–µ–º 4 –∑–Ω–∞—á–µ–Ω–∏—è.
        # –ù–∞–º –Ω—É–∂–µ–Ω –∏–º–µ–Ω–Ω–æ mel_post –¥–ª—è —Ñ–∏–Ω–∞–ª—å–Ω–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞.
        mel_raw, mel_post, stop_output, attentions = student_model(tokens, lens, speaker_embs=spk_emb)

    # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention (–±–µ—Ä–µ–º –∏–∑ —Ç–æ–≥–æ –∂–µ –º–µ—Å—Ç–∞)
    save_attention_image(attentions[0], "inference_attention.png")

    # 4. –õ–æ–≥–∏–∫–∞ Stop Token
    # 4. –õ–æ–≥–∏–∫–∞ Stop Token
    stop_probs = torch.sigmoid(stop_output[0]).cpu().numpy()  # [Time, 1]

    # –î–û–ë–ê–í–¨ –≠–¢–û: –ü–æ—Å–º–æ—Ç—Ä–∏–º, –Ω–∞—Å–∫–æ–ª—å–∫–æ –º–æ–¥–µ–ª—å –≤–æ–æ–±—â–µ —É–≤–µ—Ä–µ–Ω–∞
    print(f"üìä –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω–∞ –∑–∞ –≤–µ—Å—å —Ñ–∞–π–ª: {stop_probs.max():.4f}")

    stop_threshold = 0.5  # <--- –°–ù–ò–ó–ò–õ–ò –ü–û–†–û–ì (–±—ã–ª–æ 0.5)
    min_frames = 50  # –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è —Ä–∞–Ω—å—à–µ ~0.5 —Å–µ–∫

    stop_indices = np.where(stop_probs[min_frames:] > stop_threshold)[0]

    # –†–∞–±–æ—Ç–∞–µ–º —Ç–µ–ø–µ—Ä—å —Å mel_post
    final_mel = mel_post

    if len(stop_indices) > 0:
        cut_idx = stop_indices[0] + min_frames
        print(f"‚úÇÔ∏è –û–±—Ä–µ–∑–∫–∞ –ø–æ Stop Token –Ω–∞ –∫–∞–¥—Ä–µ {cut_idx}")
        final_mel = final_mel[:, :cut_idx, :]
    else:
        print("‚ö†Ô∏è Stop Token –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª –≤–æ –≤–Ω–µ—à–Ω–µ–º —Ü–∏–∫–ª–µ, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥–ª–∏–Ω—É –∏–∑ –º–æ–¥–µ–ª–∏.")

    # 5. –°–∏–Ω—Ç–µ–∑ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ Vocos
    print("üîä –°–∏–Ω—Ç–µ–∑ –∞—É–¥–∏–æ (Vocos) –∏–∑ Post-Net –≤—ã—Ö–æ–¥–∞...")

    # –¢—Ä–∞–Ω—Å–ø–æ–Ω–∏—Ä—É–µ–º –∏–º–µ–Ω–Ω–æ —Ñ–∏–Ω–∞–ª—å–Ω—ã–π (—É–ª—É—á—à–µ–Ω–Ω—ã–π) –º–µ–ª
    features = final_mel.transpose(1, 2)

    with torch.no_grad():
        wav = vocoder.decode(features)
        wav = wav.squeeze().cpu().numpy()

    sf.write(output_path, wav, 24000)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {output_path}")


if __name__ == "__main__":
    # –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ª—É—á—à–µ CPU, –µ—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—Ç–∫–æ–µ, –∏–ª–∏ GPU)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    cfg = Config()
    cfg.speaker_embedding_dim = 192  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å SpeechBrain

    tp = TextProcessor(cfg.RUS_ALPHABET)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –°—Ç—É–¥–µ–Ω—Ç–∞
    student = StudentTTS(cfg).to(device)

    # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ –ù–û–í–û–ú–£ —á–µ–∫–ø–æ–∏–Ω—Ç—É (–æ–±—É—á–µ–Ω–Ω–æ–º—É –Ω–∞ Vocos –¥–∞–Ω–Ω—ã—Ö)
    # –°—Ç–∞—Ä—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã (–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ librosa) —Ä–∞–±–æ—Ç–∞—Ç—å –ù–ï –ë–£–î–£–¢
    ckpt_path = "checkpoints/student_step_27250.pth"  # <--- –ü–û–ú–ï–ù–Ø–ô –ù–ê –°–í–û–ô

    if os.path.exists(ckpt_path):
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {ckpt_path}")
        ckpt = torch.load(ckpt_path, map_location=device)
        student.load_state_dict(ckpt['model_state_dict'])
    else:
        print(f"‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç {ckpt_path} –Ω–µ –Ω–∞–π–¥–µ–Ω! –ë—É–¥–µ—Ç —à—É–º.")

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –í–æ–∫–æ–¥–µ—Ä–∞ –∏ –≠–Ω–∫–æ–¥–µ—Ä–∞
    vocoder, spk_encoder = load_models(cfg, device=device)

    # 3. –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    test_text = "–ü—Ä–∏–≤–µ—Ç –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–∞."

    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≥–æ–ª–æ—Å–æ–º (–ª—é–±–æ–π wav/mp3)
    # ref_audio = r"C:\Users\light\Downloads\podcasts_1_stripped_archive\podcasts_1_stripped\test\100605980\100605980_1.mp3"
    ref_audio = "samples/audio_2026-02-16_01-29-54.wav"
    # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–¥–∏–º —à—É–º –¥–ª—è —Ç–µ—Å—Ç–∞ (—á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ —É–ø–∞–ª)
    if not os.path.exists(ref_audio):
        print("–°–æ–∑–¥–∞—é –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≥–æ–ª–æ—Å–∞ –¥–ª—è —Ç–µ—Å—Ç–∞...")
        sf.write(ref_audio, np.random.uniform(-0.5, 0.5, 16000 * 3), 16000)
    # 4. –ó–∞–ø—É—Å–∫
    generate_zero_shot(
        student,
        vocoder,
        spk_encoder,
        test_text,
        ref_audio,
        cfg,
        tp,
        output_path="result_vocos3.wav",
        device=device
    )