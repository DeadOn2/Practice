import torch
import torchaudio
import numpy as np
import soundfile as sf
import os
import matplotlib.pyplot as plt
from vocos import Vocos
from speechbrain.inference.speaker import EncoderClassifier

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Ç–≤–æ–∏ –∫–ª–∞—Å—Å—ã –∏–∑ —Ñ–∞–π–ª–∞ –æ–±—É—á–µ–Ω–∏—è
# –£–±–µ–¥–∏—Å—å, —á—Ç–æ —Ñ–∞–π–ª –Ω–∞–∑—ã–≤–∞–µ—Ç—Å—è GigaTestLSTM.py –∏–ª–∏ –∏–∑–º–µ–Ω–∏ –∏–º–ø–æ—Ä—Ç
from GigaTestLSTM import Config, TextProcessor, StudentTTS


# ==========================================
# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
# ==========================================
def load_models(cfg, device="cpu"):
    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Vocos –Ω–∞ {device}...")
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç—É –∂–µ –º–æ–¥–µ–ª—å, —á—Ç–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª–∏ –¥–ª—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö
    vocoder = Vocos.from_pretrained("charactr/vocos-mel-24khz").to(device)
    vocoder.eval()

    print(f"‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ Speaker Encoder (ECAPA-TDNN) –Ω–∞ {device}...")
    spk_encoder = EncoderClassifier.from_hparams(
        source="speechbrain/spkrec-ecapa-voxceleb",
        run_opts={"device": device}
    )
    return vocoder, spk_encoder


# ==========================================
# 2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∞ (SpeechBrain)
# ==========================================
def extract_speaker_embedding(audio_path, encoder, device):
    # SpeechBrain ECAPA-TDNN —Ç—Ä–µ–±—É–µ—Ç 16000 Hz
    signal, fs = torchaudio.load(audio_path)

    if fs != 16000:
        resampler = torchaudio.transforms.Resample(orig_freq=fs, new_freq=16000)
        signal = resampler(signal)

    # –ï—Å–ª–∏ —Å—Ç–µ—Ä–µ–æ -> –º–æ–Ω–æ
    if signal.shape[0] > 1:
        signal = torch.mean(signal, dim=0, keepdim=True)

    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –≥—Ä–æ–º–∫–æ—Å—Ç–∏ –ø–µ—Ä–µ–¥ —ç–Ω–∫–æ–¥–µ—Ä–æ–º (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –Ω–æ –ø–æ–ª–µ–∑–Ω–æ)
    signal = signal / torch.max(torch.abs(signal))

    with torch.no_grad():
        # signal –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –Ω–∞ —Ç–æ–º –∂–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–µ, —á—Ç–æ –∏ —ç–Ω–∫–æ–¥–µ—Ä
        emb = encoder.encode_batch(signal.to(device))
        return emb.squeeze(1)  # [1, 192]


# ==========================================
# 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention
# ==========================================
def save_attention_image(attn, path="debug_attention.png"):
    plt.figure(figsize=(10, 6))
    plt.imshow(attn.cpu().numpy(), aspect='auto', origin='lower', interpolation='none')
    plt.xlabel("Encoder steps (Text)")
    plt.ylabel("Decoder steps (Audio)")
    plt.title("Attention Map")
    plt.colorbar()
    plt.tight_layout()
    plt.savefig(path)
    plt.close()
    print(f"üîç –ö–∞—Ä—Ç–∞ –≤–Ω–∏–º–∞–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {path}")


# ==========================================
# 4. –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
# ==========================================
def generate_zero_shot(
        student_model,
        vocoder,
        spk_encoder,
        text,
        ref_audio_path,
        cfg,
        processor,
        output_path="zero_shot_result.wav",
        device="cpu"
):
    student_model.eval()
    student_model.to(device)

    # 1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ–∫—Å—Ç–∞
    tokens = torch.tensor([processor.encode(text)], dtype=torch.long).to(device)
    lens = torch.tensor([tokens.size(1)]).to(device)
    print(tokens)
    # 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –≥–æ–ª–æ—Å–∞
    print(f"üé§ –ß–∏—Ç–∞–µ–º –≥–æ–ª–æ—Å –∏–∑: {ref_audio_path}")
    spk_emb = extract_speaker_embedding(ref_audio_path, spk_encoder, device)

    print("ü§ñ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–µ–∫—Ç—Ä–æ–≥—Ä–∞–º–º—ã...")
    with torch.no_grad():
        # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
        # mel_output: [1, Time, 100]
        # stop_output: [1, Time, 1]
        mel_output, stop_output, attentions = student_model(tokens, lens, speaker_embs=spk_emb)

    # 3. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è Attention
    save_attention_image(attentions[0], "inference_attention.png")

    # 4. –õ–æ–≥–∏–∫–∞ Stop Token
    stop_probs = torch.sigmoid(stop_output[0]).cpu().numpy()  # [Time, 1]

    stop_threshold = 0.5
    min_frames = 50  # –ù–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è —Ä–∞–Ω—å—à–µ ~0.5 —Å–µ–∫

    # –ò—â–µ–º, –≥–¥–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—å –æ—Å—Ç–∞–Ω–æ–≤–∫–∏ –ø—Ä–µ–≤—ã—Å–∏–ª–∞ –ø–æ—Ä–æ–≥
    stop_indices = np.where(stop_probs[min_frames:] > stop_threshold)[0]

    if len(stop_indices) > 0:
        cut_idx = stop_indices[0] + min_frames
        print(f"‚úÇÔ∏è –û–±—Ä–µ–∑–∫–∞ –ø–æ Stop Token –Ω–∞ –∫–∞–¥—Ä–µ {cut_idx}")
        mel_output = mel_output[:, :cut_idx, :]
    else:
        print("‚ö†Ô∏è Stop Token –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ–º –ø–æ–ª–Ω—É—é –¥–ª–∏–Ω—É.")

    # 5. –°–∏–Ω—Ç–µ–∑ –∑–≤—É–∫–∞ —á–µ—Ä–µ–∑ Vocos
    print("üîä –°–∏–Ω—Ç–µ–∑ –∞—É–¥–∏–æ (Vocos)...")

    # –í–ê–ñ–ù–û:
    # –ú–æ–¥–µ–ª—å –≤—ã–¥–∞–ª–∞ [1, Time, 100]
    # Vocos –æ–∂–∏–¥–∞–µ—Ç [1, 100, Time]
    features = mel_output.transpose(1, 2)

    # –í–ù–ò–ú–ê–ù–ò–ï: –ú—ã –ù–ï –¥–µ–ª–∞–µ–º –¥–µ–Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é ((x*80)-80),
    # —Ç–∞–∫ –∫–∞–∫ –º–æ–¥–µ–ª—å —É—á–∏–ª–∞—Å—å –ø—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞—Ç—å —á–∏—Å—Ç—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ Vocos.

    with torch.no_grad():
        wav = vocoder.decode(features)
        wav = wav.squeeze().cpu().numpy()

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º (Vocos 24khz)
    sf.write(output_path, wav, 24000)
    print(f"‚úÖ –ì–æ—Ç–æ–≤–æ! –ê—É–¥–∏–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ –≤: {output_path}")


if __name__ == "__main__":
    # –í—ã–±–∏—Ä–∞–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ (–¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –ª—É—á—à–µ CPU, –µ—Å–ª–∏ –∞—É–¥–∏–æ –∫–æ—Ä–æ—Ç–∫–æ–µ, –∏–ª–∏ GPU)
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")

    cfg = Config()
    cfg.speaker_embedding_dim = 192  # –£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å SpeechBrain

    tp = TextProcessor(cfg.RUS_ALPHABET)

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –°—Ç—É–¥–µ–Ω—Ç–∞
    student = StudentTTS(cfg).to(device)

    # –£–∫–∞–∂–∏ –ø—É—Ç—å –∫ –ù–û–í–û–ú–£ —á–µ–∫–ø–æ–∏–Ω—Ç—É (–æ–±—É—á–µ–Ω–Ω–æ–º—É –Ω–∞ Vocos –¥–∞–Ω–Ω—ã—Ö)
    # –°—Ç–∞—Ä—ã–µ —á–µ–∫–ø–æ–∏–Ω—Ç—ã (–æ–±—É—á–µ–Ω–Ω—ã–µ –Ω–∞ librosa) —Ä–∞–±–æ—Ç–∞—Ç—å –ù–ï –ë–£–î–£–¢
    ckpt_path = "checkpoints/student_step_9500.pth"  # <--- –ü–û–ú–ï–ù–Ø–ô –ù–ê –°–í–û–ô

    if os.path.exists(ckpt_path):
        print(f"üìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {ckpt_path}")
        ckpt = torch.load(ckpt_path, map_location=device)
        student.load_state_dict(ckpt['model_state_dict'])
    else:
        print(f"‚ö†Ô∏è –ß–µ–∫–ø–æ–∏–Ω—Ç {ckpt_path} –Ω–µ –Ω–∞–π–¥–µ–Ω! –ë—É–¥–µ—Ç —à—É–º.")

    # 2. –ó–∞–≥—Ä—É–∑–∫–∞ –í–æ–∫–æ–¥–µ—Ä–∞ –∏ –≠–Ω–∫–æ–¥–µ—Ä–∞
    vocoder, spk_encoder = load_models(cfg, device=device)

    # 3. –î–∞–Ω–Ω—ã–µ –¥–ª—è —Ç–µ—Å—Ç–∞
    test_text = "–ü—Ä–∏–≤–µ—Ç! –≠—Ç–æ —Ç–µ—Å—Ç –Ω–æ–≤–æ–≥–æ –º–µ—Ç–æ–¥–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∑–≤—É–∫–∞."

    # –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –≥–æ–ª–æ—Å–æ–º (–ª—é–±–æ–π wav/mp3)
    ref_audio = "samples/audio_2026-02-16_01-29-54.wav"

    # –ï—Å–ª–∏ —Ñ–∞–π–ª–∞ –Ω–µ—Ç, —Å–æ–∑–¥–∞–¥–∏–º —à—É–º –¥–ª—è —Ç–µ—Å—Ç–∞ (—á—Ç–æ–±—ã –∫–æ–¥ –Ω–µ —É–ø–∞–ª)
    if not os.path.exists(ref_audio):
        print("–°–æ–∑–¥–∞—é –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –≥–æ–ª–æ—Å–∞ –¥–ª—è —Ç–µ—Å—Ç–∞...")
        sf.write(ref_audio, np.random.uniform(-0.5, 0.5, 16000 * 3), 16000)
    # 4. –ó–∞–ø—É—Å–∫
    generate_zero_shot(
        student,
        vocoder,
        spk_encoder,
        test_text,
        ref_audio,
        cfg,
        tp,
        output_path="result_vocos.wav",
        device=device
    )